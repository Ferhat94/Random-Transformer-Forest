{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501185f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import CanineForSequenceClassification, CanineTokenizer, BertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import softmax\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from sklearn.utils import resample\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4b0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    precision = load_metric(\"precision\")\n",
    "    recall = load_metric(\"recall\")\n",
    "    f1 = load_metric(\"f1\")\n",
    "    acc = load_metric(\"accuracy\")\n",
    "    mcc = load_metric(\"matthews_correlation\")\n",
    "    #auc = load_metric(\"auc\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision.compute(predictions=predictions, average = \"macro\", references=labels)[\"precision\"]\n",
    "    recall = recall.compute(predictions=predictions, average = \"macro\", references=labels)[\"recall\"]\n",
    "    f1 = f1.compute(predictions=predictions, average = \"macro\", references=labels)[\"f1\"]\n",
    "    acc = acc.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    mcc = mcc.compute(predictions=predictions, references=labels)[\"matthews_correlation\"]\n",
    "    auc = roc_auc_score(labels, softmax(logits, axis=1), multi_class='ovo', average='macro')\n",
    "    return {\"precision\": precision, \"recall\": recall, \"acc\": acc, \"mcc\": mcc, \"f1\": f1, \"auc\":auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32a6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79d7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomTransformerClassifier:\n",
    "    def __init__(self, trainingset, validationset, testset, num_classes, epochs=10, batch_size=8, model_name='google/canine-s', max_sequence_length=2048):\n",
    "        self.trainingset = trainingset\n",
    "        self.validationset = validationset\n",
    "        self.testset = testset\n",
    "        self.num_classes = num_classes\n",
    "        self.model_name = model_name\n",
    "        if 'google/canine' == model_name.split('-')[0]:\n",
    "            self.tokenizer = CanineTokenizer.from_pretrained(self.model_name)\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            \n",
    "        self.tokenizer.model_max_length = max_sequence_length\n",
    "        self.base_model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=num_classes)\n",
    "        self.training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        #do_train=True,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_auc',\n",
    "        greater_is_better=True,\n",
    "        fp16=True,\n",
    "        num_train_epochs=epochs,              # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "        lr_scheduler_type='cosine',\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        dataloader_num_workers=16,\n",
    "        #logging_dir='./logs',            # directory for storing logs\n",
    "        #logging_steps=10,\n",
    "    )\n",
    "        \n",
    "    def fit(self, n_estimators=2):\n",
    "        self.validation_preds = []\n",
    "        self.test_preds = []\n",
    "        for i in range(n_estimators):\n",
    "            #bagging_trainset = self.trainingset.sample(frac=1, replace=True)\n",
    "            bagging_trainset = resample(self.trainingset, replace=True, stratify=self.trainingset['class'])\n",
    "            train_encodings = self.tokenizer(self.get_list_strs(bagging_trainset.api), padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            val_encodings = self.tokenizer(self.get_list_strs(self.validationset.api), padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            test_encodings = self.tokenizer(self.get_list_strs(self.testset.api), padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "            \n",
    "            trainset = MalwareDataset(train_encodings, bagging_trainset['class'].values)\n",
    "            valset = MalwareDataset(val_encodings, self.validationset['class'].values)\n",
    "            testset = MalwareDataset(test_encodings, self.testset['class'].values)\n",
    "            \n",
    "            trainer = WeightedLossTrainer(\n",
    "                model=self.base_model, args=self.training_args, train_dataset=trainset, eval_dataset=valset,\n",
    "                compute_metrics=compute_metrics\n",
    "            )\n",
    "            \n",
    "            trainer.train()\n",
    "            \n",
    "            val_preds = trainer.predict(valset)\n",
    "            test_preds = trainer.predict(testset)\n",
    "            self.validation_preds.append(val_preds)\n",
    "            self.test_preds.append(test_preds)\n",
    "            del bagging_trainset\n",
    "            del train_encodings\n",
    "            del val_encodings\n",
    "            del test_encodings\n",
    "            del trainset\n",
    "            del valset\n",
    "            del testset\n",
    "            del trainer\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f'{i + 1}. estimator is done....')\n",
    "    def get_preds(self):\n",
    "        return self.validation_preds, self.test_preds\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        #np.argmax((softmax(val[0].predictions, axis=1) + softmax(val[1].predictions, axis=1))/2, axis=1)\n",
    "        val_preds = np.zeros_like(self.validation_preds[0].predictions)\n",
    "        for val in self.validation_preds:\n",
    "            val_preds += softmax(val.predictions, axis=1)\n",
    "        \n",
    "        val_logits = val_preds / len(self.validation_preds)\n",
    "        val_preds = np.argmax(val_preds / len(self.validation_preds), axis=1)\n",
    "        \n",
    "        test_preds = np.zeros_like(self.test_preds[0].predictions)\n",
    "        for test in self.test_preds:\n",
    "            test_preds += softmax(test.predictions, axis=1)\n",
    "        \n",
    "        test_logits = test_preds / len(self.test_preds)\n",
    "        test_preds = np.argmax(test_preds / len(self.test_preds), axis=1)\n",
    "        precision = load_metric(\"precision\")\n",
    "        recall = load_metric(\"recall\")\n",
    "        f1 = load_metric(\"f1\")\n",
    "        acc = load_metric(\"accuracy\")\n",
    "        mcc = load_metric(\"matthews_correlation\")\n",
    "    \n",
    "        val_precision = precision.compute(predictions=val_preds, average = \"macro\", references=self.validationset[\"class\"].values)[\"precision\"]\n",
    "        val_recall = recall.compute(predictions=val_preds, average = \"macro\", references=self.validationset[\"class\"].values)[\"recall\"]\n",
    "        val_f1 = f1.compute(predictions=val_preds, average = \"macro\", references=self.validationset[\"class\"].values)[\"f1\"]\n",
    "        val_acc = acc.compute(predictions=val_preds, references=self.validationset[\"class\"].values)[\"accuracy\"]\n",
    "        val_mcc = mcc.compute(predictions=val_preds, references=self.validationset[\"class\"].values)[\"matthews_correlation\"]\n",
    "        val_auc = roc_auc_score(self.validationset[\"class\"].values, softmax(val_logits, axis=1), multi_class='ovo', average='macro')\n",
    "        \n",
    "        test_precision = precision.compute(predictions=test_preds, average = \"macro\", references=self.testset[\"class\"].values)[\"precision\"]\n",
    "        test_recall = recall.compute(predictions=test_preds, average = \"macro\", references=self.testset[\"class\"].values)[\"recall\"]\n",
    "        test_f1 = f1.compute(predictions=test_preds, average = \"macro\", references=self.testset[\"class\"].values)[\"f1\"]\n",
    "        test_acc = acc.compute(predictions=test_preds, references=self.testset[\"class\"].values)[\"accuracy\"]\n",
    "        test_mcc = mcc.compute(predictions=test_preds, references=self.testset[\"class\"].values)[\"matthews_correlation\"]\n",
    "        test_auc = roc_auc_score(self.testset[\"class\"].values, softmax(test_logits, axis=1), multi_class='ovo', average='macro')\n",
    "\n",
    "        return {\"val_precision\": val_precision, \"val_recall\": val_recall, \"val_acc\": val_acc, \"val_mcc\": val_mcc, \"val_f1\": val_f1, \"val_auc\":val_auc}, {\"test_precision\": test_precision, \"test_recall\": test_recall, \"test_acc\": test_acc, \"test_mcc\": test_mcc, \"test_f1\": test_f1, \"test_auc\":test_auc}\n",
    "        \n",
    "\n",
    "        \n",
    "    def get_list_strs(self, df):\n",
    "        lst_str = []\n",
    "        for i in range(len(df)):\n",
    "            str_ = df.values[i]\n",
    "            lst_str.append(str_)\n",
    "        return lst_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0637cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "def get_conf_mat(preds, truth):\n",
    "    test_preds = np.zeros_like(preds[0].predictions)\n",
    "    for test in preds:\n",
    "        test_preds += softmax(test.predictions, axis=1)\n",
    "    test_logits = test_preds / len(preds)\n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "    f1 = f1_score(truth, test_preds, average= \"macro\")\n",
    "    return confusion_matrix(truth, test_preds), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ccdf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_calls_df = pd.read_csv(\"../Datasets/Oliveria.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e5eae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_calls_df.columns = [\"api\", \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63108220",
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f87ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_calls_df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "209cb3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT2IDX = {\n",
    "    'Trojan': 0,\n",
    "    'Adware': 1,\n",
    "    'Downloader': 2,\n",
    "    'Ransomware': 3,\n",
    "    'Agent': 4,\n",
    "    'Riskware': 5,\n",
    "    'Backdoor': 6,\n",
    "    'Dropper': 7,\n",
    "    'Virus': 8\n",
    "}\n",
    "\n",
    "IDX2CAT = {\n",
    "    0:'Trojan',\n",
    "    1:'Adware',\n",
    "    2:'Downloader',\n",
    "    3:'Ransomware',\n",
    "    4:'Agent',\n",
    "    5:'Riskware',\n",
    "    6:'Backdoor',\n",
    "    7:'Dropper',\n",
    "    8:'Virus'   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb7c340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_calls_df['class'] = malware_calls_df['class'].map(lambda x: CAT2IDX[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d373f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(malware_calls_df,\n",
    "test_size=0.2, random_state=75, stratify = malware_calls_df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e3a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a95b30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X_train, test_size=0.2, random_state=75, stratify = X_train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7efecf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2986613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weights = (1 - (malware_calls_df['class'].value_counts().sort_index() / len(malware_calls_df))).values\n",
    "#class_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n",
    "#class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56309157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_func(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "076ff757",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomTransformerClassifier(X_train, X_val, X_test, num_classes=9, \n",
    "                                         epochs=20, batch_size=8, max_sequence_length=512,\n",
    "                                         model_name = \"bert-base-cased\")                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90591d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(n_estimators=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35f71b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "val, test = classifier.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4b06113",
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e4a84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868ac6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm, f1 = get_conf_mat(classifier.get_preds()[1], X_test[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dde52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          f1 = None,\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}; f1 = {:0.4f}'.format(accuracy, misclass,f1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee8b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"Trojan\",\"Adware\",\"Downloader\",\"Ransomware\",\"Agent\",\"Riskware\",\"Backdoor\",\"Dropper\",\"Virus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d21b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm,\n",
    "                      target_names,\n",
    "                      title='Confusion matrix',\n",
    "                      f1 = f1,\n",
    "                      cmap=None,\n",
    "                      normalize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maltransform",
   "language": "python",
   "name": "maltransform"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
