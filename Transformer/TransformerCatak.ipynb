{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerMultiFÖÇGithub.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7m4CGRORzEp"
      },
      "source": [
        "pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTxB57JK_6lt"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import os\n",
        "import time\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding, Flatten\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SpatialDropout1D\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axHE2OAguDTx"
      },
      "source": [
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n",
        "    \n",
        "    def get_config(self):\n",
        "        cfg = super().get_config()\n",
        "        return cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wWaAZpsuIec"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "    \n",
        "    def get_config(self):\n",
        "        cfg = super().get_config()\n",
        "        return cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqUBJTf4uMHF"
      },
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "    def get_config(self):\n",
        "        cfg = super().get_config()\n",
        "        return cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpFrEyYLAUF-"
      },
      "source": [
        "root_path = \"../datasets/\"\n",
        "os.chdir(root_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rTl4CmDAsUV"
      },
      "source": [
        "catakOriginal = pd.read_csv(\"CatakOriginal.csv\")\n",
        "#catakPreprocessed = pd.read_csv(\"CatakPreprocessed.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvgjNgVjAtsX"
      },
      "source": [
        "catakOriginal.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-nva8FoA5l9"
      },
      "source": [
        "catakOriginal[\"class\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_SsE42ZcsgH"
      },
      "source": [
        "enc = OneHotEncoder()\n",
        "Y_sparse = enc.fit_transform(np.array(catakOriginal[\"class\"]).reshape((-1,1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLM6OiM2e2mW"
      },
      "source": [
        "enc.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5W6H1ENc57M"
      },
      "source": [
        "Y = Y_sparse.todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFzvy_H7BiQI"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbPUWpX0B0Sm"
      },
      "source": [
        "vocab_size = 800\n",
        "maxlen = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_B9MV2WB2XL"
      },
      "source": [
        "X = catakOriginal.api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F42b-tysB4X7"
      },
      "source": [
        "tok = Tokenizer(num_words=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT5mA4WwoluU"
      },
      "source": [
        "tok.fit_on_texts(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1SmRDVjB-Pp"
      },
      "source": [
        "print('Found %s unique tokens.' % len(tok.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0fO4o5eB_sS"
      },
      "source": [
        "X = tok.texts_to_sequences(X.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2mmnTY_CDim"
      },
      "source": [
        "X = sequence.pad_sequences(X, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajxug4gTCEBq"
      },
      "source": [
        "print('Shape of data tensor:', X.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nljIdS_1CF0S"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
        "                                                    test_size=0.2, random_state=75, stratify = Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr47zv4hup8S"
      },
      "source": [
        "embed_dim = 16  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "#transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(100, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(Y_train.shape[1], activation=\"softmax\")(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight"
      ],
      "metadata": {
        "id": "ncTFhg5mujhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = class_weight.compute_class_weight(class_weight = 'balanced',\n",
        "                                                 classes = np.unique(list(Y_train.argmax(1).A1)),\n",
        "                                                 y = list(Y_train.argmax(1).A1))"
      ],
      "metadata": {
        "id": "j0RnGdKIujag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = {i : class_weights[i] for i in range(Y_train.shape[1])}"
      ],
      "metadata": {
        "id": "23Ef3_WpujTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight"
      ],
      "metadata": {
        "id": "CYEDQIjXujJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PR9ESilVF6-"
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-XQg7WsCek3"
      },
      "source": [
        "lossList = []; accList = []; f1List = []; aucList = []; tiList = [];  mccList = []\n",
        "count = 0\n",
        "for i, (train_index, val_index) in enumerate(skf.split(X_train, Y_train.argmax(1))):\n",
        "  training_data_X = X_train[train_index]\n",
        "  val_data_X = X_train[val_index]\n",
        "  training_data_Y = Y_train[train_index]\n",
        "  val_data_Y = Y_train[val_index]\n",
        "  filepath = f'../best_models/CatakOriginalTransformer/best_model{i}.hdf5'\n",
        "  es = EarlyStopping(monitor= \"val_auc\", mode='max', verbose=1, patience = 10)\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor= \"val_auc\", verbose=1, save_best_only=True, mode='max')\n",
        "  callbacks_list = [checkpoint,es]\n",
        "  start_time = time.time()\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(loss = 'categorical_crossentropy', \n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy',\n",
        "                         tfa.metrics.F1Score(num_classes=Y_train.shape[1],average=\"macro\"),\n",
        "                         tfa.metrics.MatthewsCorrelationCoefficient(num_classes=Y_train.shape[1], name= \"mcc\"),\n",
        "                         AUC(multi_label = True, num_labels = Y_train.shape[1],name=\"auc\")])\n",
        "  history = model.fit(training_data_X, training_data_Y, batch_size=128, epochs=20,\n",
        "                    validation_data=(val_data_X,val_data_Y), verbose=1, callbacks = callbacks_list, class_weight=weight)\n",
        "  if count == 0:\n",
        "    model.summary()\n",
        "    count += 1\n",
        "  tiList.append(time.time() - start_time)\n",
        "  model.load_weights(filepath)\n",
        "  loss, acc, f1, mcc, auc = model.evaluate(val_data_X,val_data_Y)\n",
        "  aucList.append(auc)\n",
        "  lossList.append(loss)\n",
        "  accList.append(acc)\n",
        "  mccList.append(mcc)\n",
        "  f1List.append(f1)\n",
        "print(f'mean accuracy : {np.mean(accList)}, std loss : {np.std(accList)}')\n",
        "print(f'mean loss : {np.mean(lossList)}, std loss : {np.std(lossList)}')\n",
        "print(f'mean f1 : {np.mean(f1List)}, std f1 : {np.std(f1List)}')\n",
        "print(f'mean mcc : {np.mean(mccList)}, std f1 : {np.std(mccList)}')\n",
        "print(f'mean auc : {np.mean(aucList)}, std auc : {np.std(aucList)}')\n",
        "print(f'mean time : {np.mean(tiList)}, std time : {np.std(tiList)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXuvj3e0Cgv7"
      },
      "source": [
        "bestİndex = aucList.index(max(aucList))\n",
        "filepath = f'../best_models/CatakOriginalTransformer/best_model{bestİndex}.hdf5'\n",
        "model.load_weights(filepath)\n",
        "model.evaluate(X_test,Y_test) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred_probs = model.predict(X_test)\n",
        "y_test_pred = np.argmax(y_test_pred_probs, axis = 1)\n",
        "cm = confusion_matrix(Y_test.argmax(1), y_test_pred)"
      ],
      "metadata": {
        "id": "L4-QqvLIzAjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7FNyBoYvOiW"
      },
      "source": [
        "enc.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0QQhdBxnFb"
      },
      "source": [
        "target_names = [\"Adware\",\"Backdoor\",\"Downloader\",\"Dropper\",\"Spyware\",\"Trojan\",\"Virus\",\"Worm\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPKqOvKoxe3t"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "    \"\"\"\n",
        "    given a sklearn confusion matrix (cm), make a nice plot\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "\n",
        "    target_names: given classification classes such as [0, 1, 2]\n",
        "                  the class names, for example: ['high', 'medium', 'low']\n",
        "\n",
        "    title:        the text to display at the top of the matrix\n",
        "\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "                  plt.get_cmap('jet') or plt.cm.Blues\n",
        "\n",
        "    normalize:    If False, plot the raw numbers\n",
        "                  If True, plot the proportions\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
        "                                                              # sklearn.metrics.confusion_matrix\n",
        "                          normalize    = True,                # show proportions\n",
        "                          target_names = y_labels_vals,       # list of names of the classes\n",
        "                          title        = best_estimator_name) # title of graph\n",
        "\n",
        "    Citiation\n",
        "    ---------\n",
        "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AJYVSbayHFc"
      },
      "source": [
        "plot_confusion_matrix(cm,\n",
        "                      target_names,\n",
        "                      title='Confusion matrix',\n",
        "                      cmap=None,\n",
        "                      normalize=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}